{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmNMuk4NLTz9qwGrmxFbmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/30-day-ml-challenge-/blob/main/n_gram_language_model_for_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rYJRlKTrNKO7",
        "outputId": "372ccf1b-0b92-40c3-922d-6037ea406b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do',\n",
              " 'on',\n",
              " 'her',\n",
              " 'does',\n",
              " 'it',\n",
              " \"doesn't\",\n",
              " 'ma',\n",
              " 'yours',\n",
              " 'won',\n",
              " 'not',\n",
              " 'they',\n",
              " 'were',\n",
              " 'some',\n",
              " 'his',\n",
              " \"shan't\",\n",
              " 'again',\n",
              " 'at',\n",
              " \"couldn't\",\n",
              " 'have',\n",
              " 'this',\n",
              " 'aren',\n",
              " 'further',\n",
              " 'doesn',\n",
              " 'needn',\n",
              " 'we',\n",
              " 'is',\n",
              " \"mightn't\",\n",
              " 'did',\n",
              " 'about',\n",
              " 'during',\n",
              " \"hasn't\",\n",
              " 'she',\n",
              " 'them',\n",
              " 'him',\n",
              " \"you'll\",\n",
              " 'has',\n",
              " 'can',\n",
              " 'such',\n",
              " 'through',\n",
              " 'while',\n",
              " 'there',\n",
              " 'both',\n",
              " 'the',\n",
              " 'ourselves',\n",
              " 'so',\n",
              " 'll',\n",
              " 'because',\n",
              " 'below',\n",
              " 'over',\n",
              " 'wouldn',\n",
              " 'few',\n",
              " 'to',\n",
              " 'now',\n",
              " 'into',\n",
              " 'under',\n",
              " 'most',\n",
              " 'out',\n",
              " 'other',\n",
              " \"aren't\",\n",
              " 'should',\n",
              " 'ain',\n",
              " 'just',\n",
              " 'doing',\n",
              " 'are',\n",
              " 'myself',\n",
              " 'all',\n",
              " \"weren't\",\n",
              " \"wouldn't\",\n",
              " \"you're\",\n",
              " 'only',\n",
              " 'when',\n",
              " \"you'd\",\n",
              " 'above',\n",
              " 'own',\n",
              " 'too',\n",
              " 'isn',\n",
              " 'any',\n",
              " 'yourself',\n",
              " 'had',\n",
              " 'o',\n",
              " 'its',\n",
              " 'a',\n",
              " 'until',\n",
              " 'after',\n",
              " 'i',\n",
              " 'mustn',\n",
              " 'those',\n",
              " 'yourselves',\n",
              " 'was',\n",
              " 'up',\n",
              " 'off',\n",
              " \"that'll\",\n",
              " 'against',\n",
              " 'don',\n",
              " 'will',\n",
              " 'an',\n",
              " 'no',\n",
              " 'if',\n",
              " 'once',\n",
              " 'hasn',\n",
              " 'whom',\n",
              " \"won't\",\n",
              " 'how',\n",
              " \"it's\",\n",
              " 'your',\n",
              " 'shan',\n",
              " 'from',\n",
              " 'and',\n",
              " 'nor',\n",
              " \"mustn't\",\n",
              " 'be',\n",
              " 'mightn',\n",
              " \"haven't\",\n",
              " 'hadn',\n",
              " 'he',\n",
              " 'in',\n",
              " 'couldn',\n",
              " 'you',\n",
              " 'wasn',\n",
              " 'herself',\n",
              " 'than',\n",
              " \"don't\",\n",
              " 's',\n",
              " \"you've\",\n",
              " 'what',\n",
              " 'where',\n",
              " 'hers',\n",
              " 'who',\n",
              " \"she's\",\n",
              " 'am',\n",
              " 'been',\n",
              " \"isn't\",\n",
              " 'that',\n",
              " 'down',\n",
              " 've',\n",
              " 't',\n",
              " 're',\n",
              " 'itself',\n",
              " 'y',\n",
              " 'ours',\n",
              " 'before',\n",
              " 'why',\n",
              " \"wasn't\",\n",
              " 'as',\n",
              " 'same',\n",
              " 'here',\n",
              " 'these',\n",
              " 'but',\n",
              " 'each',\n",
              " 'their',\n",
              " 'more',\n",
              " 'between',\n",
              " 'by',\n",
              " 'which',\n",
              " 'or',\n",
              " 'of',\n",
              " 'me',\n",
              " 'himself',\n",
              " 'd',\n",
              " \"didn't\",\n",
              " 'weren',\n",
              " 'haven',\n",
              " \"shouldn't\",\n",
              " 'for',\n",
              " 'didn',\n",
              " 'then',\n",
              " \"hadn't\",\n",
              " 'shouldn',\n",
              " 'having',\n",
              " 'themselves',\n",
              " \"should've\",\n",
              " 'our',\n",
              " 'my',\n",
              " 'very',\n",
              " \"needn't\",\n",
              " 'm',\n",
              " 'theirs',\n",
              " 'with',\n",
              " 'being',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~',\n",
              " '\"',\n",
              " '\"',\n",
              " '-',\n",
              " '+',\n",
              " '—',\n",
              " 'lt',\n",
              " 'rt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# @title Language modeling is the way of determining the probability of any sequence of words.\n",
        "# imports\n",
        "\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "from nltk import FreqDist\n",
        "# Import the stopwords module\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# input the reuters sentences\n",
        "sents  =reuters.sents()\n",
        "\n",
        "# write the removal characters such as : Stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—'\n",
        "string.punctuation\n",
        "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']\n",
        "removal_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams # Import the ngrams function\n",
        "# generate unigrams bigrams trigrams\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "tokenized_text=[]\n",
        "for sentence in sents:\n",
        "  sentence = list(map(lambda x:x.lower(),sentence))\n",
        "  for word in sentence:\n",
        "        if word== '.':\n",
        "            sentence.remove(word)\n",
        "        else:\n",
        "            unigram.append(word)\n",
        "\n",
        "  tokenized_text.append(sentence)\n",
        "  bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True)))\n",
        "  trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))"
      ],
      "metadata": {
        "id": "MgSWwBxeNPqX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict # Import defaultdict from collections module\n",
        "from collections import Counter\n",
        "\n",
        "def remove_stopwords(x):\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in removal_list:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return (y)\n",
        "unigram = remove_stopwords(unigram)\n",
        "bigram = remove_stopwords(bigram)\n",
        "trigram = remove_stopwords(trigram)\n",
        "\n",
        "# generate frequency of n-grams\n",
        "freq_bi = FreqDist(bigram)\n",
        "freq_tri = FreqDist(trigram)\n",
        "\n",
        "d = defaultdict(Counter)\n",
        "for a, b, c in freq_tri:\n",
        "    if(a != None and b!= None and c!= None):\n",
        "      d[a, b][c] += freq_tri[a, b, c]\n",
        "s=''\n",
        "def pick_word(counter):\n",
        "    \"Chooses a random element.\"\n",
        "    choices = list(counter.elements())\n",
        "    if choices: # Check if choices is not empty\n",
        "        return random.choice(choices)\n",
        "    else:\n",
        "        return \"\" # Return an empty string if no choices are available\n",
        "prefix = \"he\", \"said\"\n",
        "print(\" \".join(prefix))\n",
        "s = \" \".join(prefix)\n",
        "for i in range(19):\n",
        "    suffix = pick_word(d[prefix])\n",
        "    s=s+' '+suffix\n",
        "    print(s)\n",
        "    prefix = prefix[1], suffix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg3oShkuNPwK",
        "outputId": "c1416691-6982-4d1c-a0d6-9a5eac0b0f04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he said\n",
            "he said copper\n",
            "he said copper supplies\n",
            "he said copper supplies are\n",
            "he said copper supplies are large\n",
            "he said copper supplies are large enough\n",
            "he said copper supplies are large enough now\n",
            "he said copper supplies are large enough now to\n",
            "he said copper supplies are large enough now to cope\n",
            "he said copper supplies are large enough now to cope with\n",
            "he said copper supplies are large enough now to cope with the\n",
            "he said copper supplies are large enough now to cope with the government\n",
            "he said copper supplies are large enough now to cope with the government had\n",
            "he said copper supplies are large enough now to cope with the government had not\n",
            "he said copper supplies are large enough now to cope with the government had not expected\n",
            "he said copper supplies are large enough now to cope with the government had not expected to\n",
            "he said copper supplies are large enough now to cope with the government had not expected to remain\n",
            "he said copper supplies are large enough now to cope with the government had not expected to remain pessimistic\n",
            "he said copper supplies are large enough now to cope with the government had not expected to remain pessimistic in\n",
            "he said copper supplies are large enough now to cope with the government had not expected to remain pessimistic in coming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LpkWh85rNPzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owjZF-49NP2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}